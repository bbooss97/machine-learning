{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../drive/My Drive/hw2ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the kaggle json file in the content directory before running this cell\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d ibrahimserouis99/one-piece-image-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile('one-piece-image-classifier.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames=[\n",
    "\"Ace\",\n",
    "\"Akainu\",\n",
    "\"Brook\" ,\n",
    "\"Chopper\",\n",
    "\"Crocodile\",\n",
    "\"Franky\",\n",
    "\"Jinbei\",\n",
    "\"Kurohige\",\n",
    "\"Law\",\n",
    "\"Luffy\",\n",
    "\"Mihawk\",\n",
    "\"Nami\",\n",
    "\"Rayleigh\",\n",
    "\"Robin\",\n",
    "\"Sanji\",\n",
    "\"Shanks\",\n",
    "\"Usopp\",\n",
    "\"Zoro\"]\n",
    "\n",
    "classes={name:i for i, name in enumerate(classnames)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data folder and create a file with the annotations\n",
    "import os \n",
    "\n",
    "with open(\"./annotations.txt\", \"w\") as f:\n",
    "    for i,n in enumerate(classes):\n",
    "        for file in os.listdir(\"./Data/Data/\"+n):\n",
    "            f.write(\"./Data/Data/\"+n+\"/\"+file+\" \"+str(i)+\"\\n\")\n",
    "    print(n+\" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class OnePieceDataset(Dataset):\n",
    "    items=[]\n",
    "    def __init__(self,w,h):\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        with open(\"./annotations.txt\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                self.items.append(line.split(\" \"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item=self.items[idx]\n",
    "\n",
    "        #one hot encoding of the class of the item\n",
    "        label=torch.zeros(len(classes))\n",
    "        label[int(item[-1].strip())]=1\n",
    "        \n",
    "        #load the image as pil image\n",
    "        image=Image.open(\" \".join(item[0:-1])).convert('RGB')\n",
    "        image = image.resize((self.w, self.h)) \n",
    "\n",
    "        #convert it to a tensor\n",
    "        image=torch.tensor(np.array(image),dtype=torch.float32)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self,w,h):\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(self.w*self.h*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc3(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc4(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc5(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc6(x)\n",
    "        \n",
    "        #do not need to use softmax or sigmoid because we use cross entropy loss and it does it for us\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import OnePieceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from nn import Mlp\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "#device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#declare parameters\n",
    "num_epochs=5\n",
    "batch_size=32\n",
    "w,h=50,50\n",
    "w_and_b=True\n",
    "nn_type=\"mlp\"\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.init(project='hw2ml', entity='bbooss97',name=nn_type)\n",
    "\n",
    "#read the dataset\n",
    "dataset=OnePieceDataset(w,h)\n",
    "\n",
    "#split in train and test set\n",
    "split=[int(0.8*len(dataset)),int(0.2*len(dataset))+1]\n",
    "train,test = torch.utils.data.random_split(dataset,split)\n",
    "\n",
    "#dataloader\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True , drop_last=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True , drop_last=True)\n",
    "\n",
    "#types where have to change the input in the train and test\n",
    "typesToChange=[\"resnetFrom0\",\"resnetPretrainedFineTuneFc\",\"resnetPretrainedFineTuneAll\",\"mobilenetPretrainedFineTuneAll\"]\n",
    "\n",
    "#define the model\n",
    "if nn_type==\"mlp\":\n",
    "    model=Mlp(w,h)\n",
    "elif nn_type==\"resnetPretrainedFineTuneFc\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "    toFreeze=[j for i,j in model.named_parameters()][:-2]\n",
    "    for i in toFreeze:\n",
    "        i.requires_grad=False\n",
    "elif nn_type==\"resnetPretrainedFineTuneAll\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "elif nn_type==\"resnetFrom0\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18',pretrained=False)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "elif nn_type==\"mobilenetPretrainedFineTuneAll\":\n",
    "    model=torchvision.models.mobilenet_v3_small()\n",
    "    model.classifier[3]=torch.nn.Linear(1024,18)\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.watch(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#define loss and the optimizer\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #train\n",
    "    model.train()\n",
    "    if w_and_b:\n",
    "        wandb.log({\"epoch\":epoch})\n",
    "\n",
    "    #train loop\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        #move the data to the device\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        #reshape the images\n",
    "        images=images.reshape(batch_size,-1)\n",
    "\n",
    "        #change the type of the input\n",
    "        if nn_type in typesToChange:\n",
    "            images =images.reshape(batch_size,50,50,3)\n",
    "            images=torch.einsum(\"abcd->adbc\",images)\n",
    "\n",
    "        #forward pass\n",
    "        outputs=model(images)\n",
    "        \n",
    "        #calculate the loss\n",
    "        l=loss(outputs,labels)\n",
    "        \n",
    "        #backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print the loss\n",
    "        print(\"epoch: {}/{}, step: {}/{}, loss: {}\".format(epoch+1,num_epochs,i+1,len(train_dataloader),l.item()))\n",
    "        if w_and_b:\n",
    "            if i%20==0:\n",
    "                wandb.log({\"loss_train\":l.item()})\n",
    "        \n",
    "    #test\n",
    "    model.eval()\n",
    "    # wandb.watch(model)\n",
    "\n",
    "    # Initialize variables to store metrics\n",
    "    l = 0.0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Loop over the data in the test set\n",
    "    with torch.no_grad():\n",
    "        for i,(images, labels) in enumerate(test_dataloader):\n",
    "\n",
    "            # Move the data to the device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Reshape the images\n",
    "            images=images.reshape(batch_size,-1)\n",
    "\n",
    "            #change the input for those models\n",
    "            if nn_Type in typesToChange:\n",
    "                images =images.reshape(batch_size,50,50,3)\n",
    "                images=torch.einsum(\"abcd->adbc\",images)\n",
    "\n",
    "            # Forward pass: compute predictions and loss\n",
    "            outputs = model(images)\n",
    "            ls = loss(outputs, labels)\n",
    "\n",
    "            # Compute running metrics\n",
    "            l += ls.item()\n",
    "            accuracy += (outputs.argmax(dim=1) == labels.argmax(dim=1)).float().mean().item()\n",
    "\n",
    "    # Compute average metrics\n",
    "    avg_loss = l / len(test_dataloader)\n",
    "    avg_accuracy = accuracy / len(test_dataloader)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f'Test loss: {avg_loss:.4f}')\n",
    "    print(f'Test accuracy: {avg_accuracy:.4f}')\n",
    "    if w_and_b:\n",
    "        wandb.log({\"avg_loss_test\":avg_loss,\"avg_accuracy_test\":avg_accuracy})\n",
    "\n",
    "#save the model\n",
    "torch.save(model,\"./hw2/models/\"+nn_type+\".pt\")\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1rc1 (tags/v3.9.1rc1:88db374, Nov 24 2020, 19:40:11) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a067def7bdf17a53bfef7d537c9e500ff74a300ce2cf52bf8859a9dc495ac604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
