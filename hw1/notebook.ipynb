{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "dataset=pd.read_csv(\"train_set.tsv\", sep='\\t', header=0)\n",
    "dataset_numpy=dataset.to_numpy()\n",
    "x=dataset_numpy[:,:-2]\n",
    "y=dataset_numpy[:,-2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x=scaler.transform(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 35) (750, 2) (250, 35) (250, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to undersample the data doenst work because i've not too much data \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "over_sampler = RandomUnderSampler(random_state=42)\n",
    "x_train, y_train = over_sampler.fit_resample(x_train, y_train[:,0])\n",
    "y_train=y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to oversample i dont think it's a good idea because i've not too much data \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "x_train, y_train = over_sampler.fit_resample(x_train, y_train[:,0])\n",
    "y_train=y_train.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate label weights\n",
    "from collections import Counter\n",
    "c=Counter(y_train[:,0])\n",
    "for k,v in c.items():\n",
    "    c[k]=v/len(y_train)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationResults(x_train,x_test,y_train,y_test,model):\n",
    "    if y_train.shape[1]>1:\n",
    "        model.fit(x_train,y_train[:,0])\n",
    "        y_pred_train=model.predict(x_train)\n",
    "        y_pred_test=model.predict(x_test)\n",
    "        print(\"Train F1 score: \",f1_score(y_train[:,0],y_pred_train,average='micro'))\n",
    "        print(\"Train Precision score: \",precision_score(y_train[:,0],y_pred_train,average='micro'))\n",
    "        print(\"Train Recall score: \",recall_score(y_train[:,0],y_pred_train,average='micro'))\n",
    "        print(\"Test F1 score: \",f1_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"Test Precision score: \",precision_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"Test Recall score: \",recall_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"\\nConfusion Matrix test:\")\n",
    "\n",
    "        cm = confusion_matrix(y_test[:,0], y_pred_test)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "    else:\n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred_train=model.predict(x_train)\n",
    "        y_pred_test=model.predict(x_test)\n",
    "        print(\"Train F1 score: \",f1_score(y_train,y_pred_train,average='micro'))\n",
    "        print(\"Train Precision score: \",precision_score(y_train,y_pred_train,average='micro'))\n",
    "        print(\"Train Recall score: \",recall_score(y_train,y_pred_train,average='micro'))\n",
    "        print(\"Test F1 score: \",f1_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"Test Precision score: \",precision_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"Test Recall score: \",recall_score(y_test[:,0],y_pred_test,average='micro'))\n",
    "        print(\"\\nConfusion Matrix test:\")\n",
    "\n",
    "        cm = confusion_matrix(y_test[:,0], y_pred_test)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionResults(x_train,x_test,y_train,y_test,model):\n",
    "    model.fit(x_train,y_train[:,1])\n",
    "    y_pred_train=model.predict(x_train)\n",
    "    y_pred_test=model.predict(x_test)\n",
    "    print(\"Train MSE: \",np.mean((y_train[:,1]-y_pred_train)**2))\n",
    "    print(\"Train MAE: \",np.mean(np.abs(y_train[:,1]-y_pred_train)))\n",
    "    print(\"Test MSE: \",np.mean((y_test[:,1]-y_pred_test)**2))\n",
    "    print(\"Test MAE: \",np.mean(np.abs(y_test[:,1]-y_pred_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 100, 'n_estimators': 50}\n",
      "0.552\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "\n",
    "# >>> from sklearn import svm, datasets\n",
    "# >>> from sklearn.model_selection import GridSearchCV\n",
    "# >>> iris = datasets.load_iris()\n",
    "# >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# >>> svc = svm.SVC()\n",
    "# >>> clf = GridSearchCV(svc, parameters)\n",
    "# >>> clf.fit(iris.data, iris.target)\n",
    "# GridSearchCV(estimator=SVC(),\n",
    "#              param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
    "# >>> sorted(clf.cv_results_.keys())\n",
    "# ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
    "#  'param_C', 'param_kernel', 'params',...\n",
    "#  'rank_test_score', 'split0_test_score',...\n",
    "#  'split2_test_score', ...\n",
    "#  'std_fit_time', 'std_score_time', 'std_test_score']\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters={'n_estimators':[10,50,100,200,500],'max_depth':[5,10,20,50,100],\\\n",
    "            'min_samples_split':[2,5,10,20,50],'min_samples_leaf':[1,2,5,10,20],\\\n",
    "            'criterion':['gini','entropy',\"log_loss\"]}\n",
    "\n",
    "\n",
    "model=RandomForestClassifier()\n",
    "\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(x_train, y_train[:,0])\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import tree\n",
    "model=tree.DecisionTreeClassifier()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)\n",
    "\n",
    "#there is a lot of overfitting because decision trees try to memorize the dataset so i try to use random forest to tackle this problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(max_iter=1000)\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model=GaussianNB()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andrea\\Desktop\\machine-learning\\hw1\\notebook.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/machine-learning/hw1/notebook.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m=\u001b[39msvm\u001b[39m.\u001b[39mSVC()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/machine-learning/hw1/notebook.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m clf \u001b[39m=\u001b[39m GridSearchCV(model, parameters)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/machine-learning/hw1/notebook.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train, y_train[:,\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/machine-learning/hw1/notebook.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(clf\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/machine-learning/hw1/notebook.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(clf\u001b[39m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m extra_args \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, arg)\n\u001b[0;32m     67\u001b[0m             \u001b[39mfor\u001b[39;00m name, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[0;32m     68\u001b[0m                                  args[\u001b[39m-\u001b[39mextra_args:])]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    835\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    836\u001b[0m         all_candidate_params, n_splits, all_out,\n\u001b[0;32m    837\u001b[0m         all_more_results)\n\u001b[0;32m    839\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 841\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    843\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    845\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1296\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1295\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:795\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    791\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    793\u001b[0m               n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits))\n\u001b[1;32m--> 795\u001b[0m out \u001b[39m=\u001b[39m parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0;32m    796\u001b[0m                                        X, y,\n\u001b[0;32m    797\u001b[0m                                        train\u001b[39m=\u001b[39;49mtrain, test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    798\u001b[0m                                        parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    799\u001b[0m                                        split_progress\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    800\u001b[0m                                            split_idx,\n\u001b[0;32m    801\u001b[0m                                            n_splits),\n\u001b[0;32m    802\u001b[0m                                        candidate_progress\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    803\u001b[0m                                            cand_idx,\n\u001b[0;32m    804\u001b[0m                                            n_candidates),\n\u001b[0;32m    805\u001b[0m                                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs)\n\u001b[0;32m    806\u001b[0m                \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters),\n\u001b[0;32m    807\u001b[0m                    (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    808\u001b[0m                    \u001b[39menumerate\u001b[39;49m(candidate_params),\n\u001b[0;32m    809\u001b[0m                    \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))))\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    813\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    814\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1044\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1044\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1045\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1048\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:859\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 859\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:777\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    776\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 777\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    778\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py:222\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    221\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 222\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    596\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    597\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    600\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    601\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    602\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:226\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 226\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    227\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples, )\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:277\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    271\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    273\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support, \\\n\u001b[0;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA, \\\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_ \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    278\u001b[0m         X, y,\n\u001b[0;32m    279\u001b[0m         svm_type\u001b[39m=\u001b[39;49msolver_type, sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    280\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight_, kernel\u001b[39m=\u001b[39;49mkernel, C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    281\u001b[0m         nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu, probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability, degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    282\u001b[0m         shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking, tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    283\u001b[0m         cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size, coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    284\u001b[0m         gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma, epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    285\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter, random_seed\u001b[39m=\u001b[39;49mrandom_seed)\n\u001b[0;32m    287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "parameters={'kernel':('linear', 'poly' ,\"rbf\",\"sigmoid\"), 'C':[1, 10],'gamma':[\"scale\",\"auto\"],'degree':[1,2,3,4,5],\\\n",
    "'coef0':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'shrinking':[True,False],'probability':[True,False],'tol':[0.001,0.0001,0.00001],\\\n",
    "    'cache_size':[200,500,1000],'class_weight':[None,\"balanced\"],'verbose':[True,False],'max_iter':[-1,1000,10000,100000]}\n",
    "\n",
    "\n",
    "model=svm.SVC()\n",
    "\n",
    "clf = GridSearchCV(model, parameters)\n",
    "clf.fit(x_train, y_train[:,0])\n",
    "\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model=AdaBoostClassifier()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model=GradientBoostingClassifier()\n",
    "classificationResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tree.DecisionTreeRegressor()\n",
    "regressionResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model=RandomForestRegressor()\n",
    "regressionResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression()\n",
    "regressionResults(x_train,x_test,y_train,y_test,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import support vector regression\n",
    "from sklearn.svm import SVR\n",
    "model=SVR()\n",
    "regressionResults(x_train,x_test,y_train,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model=make_pipeline(PolynomialFeatures(2),LinearRegression())\n",
    "regressionResults(x_train,x_test,y_train,y_test,model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05c73f36370b37d7956a45e693a83b97388709ec0b54883dc202775cd3dcb887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
