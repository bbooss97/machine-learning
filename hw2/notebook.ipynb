{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those first functions are to use google colab if you want to connect to it to use the dataset and install wandb and login to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../drive/My Drive/hw2ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from kaggle on google drive or on the local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert the kaggle json file in the content directory before running this cell\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d ibrahimserouis99/one-piece-image-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile('one-piece-image-classifier.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames=[\n",
    "\"Ace\",\n",
    "\"Akainu\",\n",
    "\"Brook\" ,\n",
    "\"Chopper\",\n",
    "\"Crocodile\",\n",
    "\"Franky\",\n",
    "\"Jinbei\",\n",
    "\"Kurohige\",\n",
    "\"Law\",\n",
    "\"Luffy\",\n",
    "\"Mihawk\",\n",
    "\"Nami\",\n",
    "\"Rayleigh\",\n",
    "\"Robin\",\n",
    "\"Sanji\",\n",
    "\"Shanks\",\n",
    "\"Usopp\",\n",
    "\"Zoro\"]\n",
    "\n",
    "classes={name:i for i, name in enumerate(classnames)}\n",
    "fromIndexToClass={i:name for i, name in enumerate(classnames)}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the annotation file to read the images and their labels after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data folder and create a file with the annotations\n",
    "import os \n",
    "\n",
    "with open(\"./annotations.txt\", \"w\") as f:\n",
    "    for i,n in enumerate(classes):\n",
    "        for file in os.listdir(\"./Data/Data/\"+n):\n",
    "            f.write(\"./Data/Data/\"+n+\"/\"+file+\" \"+str(i)+\"\\n\")\n",
    "    print(n+\" done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the dataset to be used in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#set the seeds to make everything reproducible\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#dataset class\n",
    "class OnePieceDataset(Dataset):\n",
    "    items=[]\n",
    "    def __init__(self,w,h):\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        with open(\"./annotations.txt\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                self.items.append(line.split(\" \"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item=self.items[idx]\n",
    "\n",
    "        #one hot encoding of the class of the item\n",
    "        label=torch.zeros(len(classes))\n",
    "        label[int(item[-1].strip())]=1\n",
    "        \n",
    "        #path of the image\n",
    "        path=\" \".join(item[0:-1])\n",
    "\n",
    "        #load the image as pil image\n",
    "        image=Image.open(path).convert('RGB')\n",
    "        image = image.resize((self.w, self.h)) \n",
    "\n",
    "        #convert it to a tensor\n",
    "        image=torch.tensor(np.array(image),dtype=torch.float32)\n",
    "        \n",
    "        return image, label ,path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a custom neural network to be used called mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self,w,h):\n",
    "        self.w=w\n",
    "        self.h=h\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(self.w*self.h*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc3(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc4(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc5(x)\n",
    "        x=nn.ReLU()(x)\n",
    "\n",
    "        x=self.fc6(x)\n",
    "        \n",
    "        #do not need to use softmax or sigmoid because we use cross entropy loss and it does it for us\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the dataset and split it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "w,h=200,200\n",
    "\n",
    "#read the dataset\n",
    "dataset=OnePieceDataset(w,h)\n",
    "\n",
    "#split in train and test set\n",
    "split=[int(0.8*len(dataset)),int(0.2*len(dataset))+1]\n",
    "train,test = torch.utils.data.random_split(dataset,split)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop to train the model and see the performances\n",
    "if you dont want to use wandb set the variable to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device= torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#declare parameters\n",
    "percentageOfDataset=1\n",
    "num_epochs=10\n",
    "batch_size=32\n",
    "w_and_b=True\n",
    "nn_type=\"resnetFrom0\"\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.init(project='hw2ml', entity='bbooss97',name=nn_type)\n",
    "\n",
    "#types where have to change the input in the train and test\n",
    "typesToChange=[\"resnetFrom0\",\"resnetPretrainedFineTuneFc\",\"resnetPretrainedFineTuneAll\",\"mobilenetPretrainedFineTuneAll\"]\n",
    "\n",
    "#dataloader\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True , drop_last=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True , drop_last=True)\n",
    "\n",
    "\n",
    "#define the model\n",
    "if nn_type==\"mlp\":\n",
    "    model=Mlp(w,h)\n",
    "elif nn_type==\"cnn\":\n",
    "    model=Cnn(w,h)\n",
    "elif nn_type==\"resnetPretrainedFineTuneFc\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "    toFreeze=[j for i,j in model.named_parameters()][:-2]\n",
    "    for i in toFreeze:\n",
    "        i.requires_grad=False\n",
    "elif nn_type==\"resnetPretrainedFineTuneAll\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "elif nn_type==\"resnetFrom0\":\n",
    "    model=torch.hub.load('pytorch/vision:v0.6.0', 'resnet18',pretrained=False)\n",
    "    model.fc=torch.nn.Linear(512,18)\n",
    "elif nn_type==\"mobilenetPretrainedFineTuneAll\":\n",
    "    model=torchvision.models.mobilenet_v3_small()\n",
    "    model.classifier[3]=torch.nn.Linear(1024,18)\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.watch(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#define loss and the optimizer\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #train\n",
    "    model.train()\n",
    "    if w_and_b:\n",
    "        wandb.log({\"epoch\":epoch})\n",
    "\n",
    "    #train loop\n",
    "    for i, (images, labels,paths) in enumerate(train_dataloader):\n",
    "\n",
    "        #use just a percentage of the dataset\n",
    "        if i>len(train_dataloader)*percentageOfDataset:\n",
    "            break\n",
    "\n",
    "        #move the data to the device\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        #reshape the images\n",
    "        images=images.reshape(batch_size,-1)\n",
    "\n",
    "        #change the type of the input\n",
    "        if nn_type in typesToChange:\n",
    "            images =images.reshape(batch_size,w,h,3)\n",
    "            images=torch.einsum(\"abcd->adbc\",images)\n",
    "\n",
    "        #forward pass\n",
    "        outputs=model(images)\n",
    "        \n",
    "        #calculate the loss\n",
    "        l=loss(outputs,labels)\n",
    "        \n",
    "        #backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print the loss\n",
    "        print(\"epoch: {}/{}, step: {}/{}, loss: {}\".format(epoch+1,num_epochs,i+1,int(len(train_dataloader)*percentageOfDataset),l.item()))\n",
    "        if w_and_b:\n",
    "            if i%20==0:\n",
    "                wandb.log({\"loss_train\":l.item()})\n",
    "        \n",
    "    #test\n",
    "    model.eval()\n",
    "    if w_and_b:\n",
    "        wandb.watch(model)\n",
    "\n",
    "    # Initialize variables to store metrics\n",
    "    l = 0.0\n",
    "    accuracy = 0.0\n",
    "    f1 =0.0\n",
    "    precision =0.0\n",
    "    recall =0.0\n",
    "\n",
    "    # Initialize lists to store predictions and labels\n",
    "    testOutputs=[]\n",
    "    testLabels=[]\n",
    "\n",
    "    # Loop over the data in the test set\n",
    "    with torch.no_grad():\n",
    "        for i,(images, labels,paths) in enumerate(test_dataloader):\n",
    "\n",
    "            # Move the data to the device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Reshape the images\n",
    "            images=images.reshape(batch_size,-1)\n",
    "\n",
    "            #change the input for those models\n",
    "            if nn_type in typesToChange:\n",
    "                images =images.reshape(batch_size,w,h,3)\n",
    "                images=torch.einsum(\"abcd->adbc\",images)\n",
    "\n",
    "            # Forward pass: compute predictions and loss\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            testOutputs.append(outputs)\n",
    "            testLabels.append(labels)\n",
    "\n",
    "            ls = loss(outputs, labels)\n",
    "\n",
    "            # Compute running metrics\n",
    "            l += ls.item()\n",
    "        \n",
    "\n",
    "    # Concatenate predictions and labels\n",
    "    testOutputs = torch.cat(testOutputs, dim=0)\n",
    "    testLabels = torch.cat(testLabels, dim=0)\n",
    "\n",
    "    testLabels=testLabels.cpu()\n",
    "    testOutputs=testOutputs.cpu()\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = l / len(test_dataloader)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(testLabels.argmax(dim=1), testOutputs.argmax(dim=1))\n",
    "\n",
    "    # Compute f1 score\n",
    "    f1 = f1_score(testLabels.argmax(dim=1), testOutputs.argmax(dim=1), average='macro')\n",
    "\n",
    "    # Compute precision\n",
    "    precision = precision_score(testLabels.argmax(dim=1), testOutputs.argmax(dim=1), average='macro')\n",
    "\n",
    "    # Compute recall\n",
    "    recall = recall_score(testLabels.argmax(dim=1), testOutputs.argmax(dim=1), average='macro')\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f'Test loss: {avg_loss:.4f}')\n",
    "    print(f'Test accuracy: {accuracy:.4f}')\n",
    "    print(f'Test f1: {f1:.4f}')\n",
    "    print(f'Test precision: {precision:.4f}')\n",
    "    print(f'Test recall: {recall:.4f}')\n",
    "\n",
    "    if w_and_b:\n",
    "        wandb.log({\"avg_loss_test\":avg_loss,\"accuracy_test\":accuracy,\"f1_test\":f1,\"precision_test\":precision,\"recall_test\":recall})\n",
    "\n",
    "    #save the model\n",
    "    torch.save(model,\"./\"+nn_type+\".pt\")\n",
    "\n",
    "if w_and_b:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to visualize examples of the predictions fo the images with also the ground thruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#use the models trained to predict some images from the test set\n",
    "#types can be \"mlp\",\"resnetPretrainedFineTuneFc\",\"resnetPretrainedFineTuneAll\",\"resnetFrom0\",\"mobilenetPretrainedFineTuneAll\"\n",
    "nn_type=\"resnetFrom0\"\n",
    "\n",
    "#load the model\n",
    "model=torch.load(\"./\"+nn_type+\".pt\")\n",
    "model.eval()\n",
    "\n",
    "#types where have to change the input in the train and test\n",
    "typesToChange=[\"resnetFrom0\",\"resnetPretrainedFineTuneFc\",\"resnetPretrainedFineTuneAll\",\"mobilenetPretrainedFineTuneAll\"]\n",
    "\n",
    "#load the images\n",
    "images=[]\n",
    "labels=[]\n",
    "paths=[]\n",
    "\n",
    "#how many images to predict\n",
    "howManyImages=30\n",
    "\n",
    "for i in range(howManyImages):\n",
    "    index=random.randint(0,len(test)-1)\n",
    "    images.append(test[index][0])\n",
    "    labels.append(test[index][1])\n",
    "    paths.append(test[index][2])\n",
    "\n",
    "#stack the labels\n",
    "labels =torch.stack(labels)\n",
    "\n",
    "#stack the imagess and reshape\n",
    "images =torch.stack(images)\n",
    "images= images.reshape(howManyImages,-1)\n",
    "\n",
    "#change the input for those models\n",
    "if nn_type in typesToChange:\n",
    "    images =images.reshape(howManyImages,w,h,3)\n",
    "    images=torch.einsum(\"abcd->adbc\",images)\n",
    "\n",
    "#predict\n",
    "outputs=model(images)\n",
    "outputs=outputs.argmax(dim=1)\n",
    "\n",
    "labels=labels.argmax(dim=1)\n",
    "\n",
    "#show the images\n",
    "for i in range(howManyImages):\n",
    "    \n",
    "    #show image from path\n",
    "    img = Image.open(paths[i])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    plt.title(\"predicted: \"+str(fromIndexToClass[outputs[i].item()])+\" real: \"+str(fromIndexToClass[labels[i].item()]))\n",
    "    plt.show()\n",
    "\n",
    "print(\"finished\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05c73f36370b37d7956a45e693a83b97388709ec0b54883dc202775cd3dcb887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
