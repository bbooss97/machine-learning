{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEmuyc0zhE4z",
        "outputId": "0001aa9a-bc9f-46c1-a88b-c2e90d217796"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6cRVDCmhsyO",
        "outputId": "e54d4a80-b78a-4e52-8db1-feb75078d104"
      },
      "outputs": [],
      "source": [
        "%cd ../gdrive/MyDrive/hw1ml/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDnbPCj9hGp5",
        "outputId": "65f5e7d0-afa3-4a50-fe7f-0a4eac36f477"
      },
      "outputs": [],
      "source": [
        "%ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1wHFE5NoQwz"
      },
      "source": [
        "develop on local environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpkm1vymlATX"
      },
      "outputs": [],
      "source": [
        "# Install useful stuff\n",
        "! apt install --yes ssh screen nano htop ranger git > /dev/null\n",
        "# SSH setting\n",
        "! echo \"root:carbonara\" | chpasswd\n",
        "! echo \"PasswordAuthentication yes\" > /etc/ssh/sshd_config\n",
        "! echo \"PermitUserEnvironment yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! service ssh restart > /dev/null\n",
        "# Download ngrok\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "# Run ngrok\n",
        "authtoken = \"20Apljvp3ViVnx3mzPGy8RN3VFt_4bzufXGF9Kkgm43PmRugS\"\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n",
        "! sleep 3\n",
        "# Get the address for SSH\n",
        "import requests\n",
        "from re import sub\n",
        "r = requests.get('http://localhost:4040/api/tunnels')\n",
        "str_ssh = r.json()['tunnels'][0]['public_url']\n",
        "str_ssh = sub(\"tcp://\", \"\", str_ssh)\n",
        "str_ssh = sub(\":\", \" -p \", str_ssh)\n",
        "str_ssh = \"ssh root@\" + str_ssh\n",
        "print(str_ssh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvgc3Cb86hT7"
      },
      "source": [
        "use once you open the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nWUoGCw6hUD",
        "outputId": "e178ad44-8723-481d-fa9b-503cf432b9c0"
      },
      "outputs": [],
      "source": [
        "%cd ../hw1ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this suppress the warnings just to see the confusion matrix and the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install shutup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "##At the top of the code\n",
        "import shutup;\n",
        "shutup.please()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9o3fDl6dMcX"
      },
      "source": [
        "function to print metrics and confusion matrix given a model to fit for the classfication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rKJ1-CyhDaP"
      },
      "outputs": [],
      "source": [
        "def classificationResults(x_train,x_test,y_train,y_test,models):\n",
        "    if type(models) is not list:\n",
        "        models=[models]\n",
        "    y_train=y_train[:,0]\n",
        "    y_test=y_test[:,0]\n",
        "\n",
        "    for model in models:\n",
        "        print(f\"the model is {model}\")\n",
        "        model.fit(x_train,y_train)\n",
        "        y_pred_train=model.predict(x_train)\n",
        "        y_pred_test=model.predict(x_test)\n",
        "        print(\"train report\")\n",
        "        print(classification_report(y_train, y_pred_train))\n",
        "        print(\"test report\")\n",
        "        print(classification_report(y_test, y_pred_test))\n",
        "        print(\"\\nConfusion Matrix test:\")\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred_test)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "        disp.plot()\n",
        "        print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHHAGAgcdS1F"
      },
      "source": [
        "function to print the metrics for the regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP0bhe6qhDaQ"
      },
      "outputs": [],
      "source": [
        "def regressionResults(x_train,x_test,y_train,y_test,models,sample_weight=None):\n",
        "    if type(models) is not list:\n",
        "        models=[models]\n",
        "    y_train=y_train[:,1]\n",
        "    y_test=y_test[:,1]\n",
        "    for model in models:\n",
        "        print(f\"the model is {model}\")\n",
        "        model.fit(x_train,y_train)\n",
        "        y_pred_train=model.predict(x_train)\n",
        "        y_pred_test=model.predict(x_test)\n",
        "        print(\"Train MSE: \",np.mean((y_train-y_pred_train)**2))\n",
        "        print(\"Train MAE: \",np.mean(np.abs(y_train-y_pred_train)))\n",
        "        print(\"Train R2: \",r2_score(y_train,y_pred_train))\n",
        "        print(\"Test MSE: \",np.mean((y_test-y_pred_test)**2))\n",
        "        print(\"Test MAE: \",np.mean(np.abs(y_test-y_pred_test)))\n",
        "        print(\"Test R2: \",r2_score(y_test,y_pred_test))\n",
        "        print(\"\\n\\n\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGsNEBuEckxV"
      },
      "source": [
        "read the dataset and divide x and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0At4GZMPhDaM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "#make everything reproducible\n",
        "random.seed(50)\n",
        "np.random.seed(50)\n",
        "\n",
        "\n",
        "dataset=pd.read_csv(\"train_set.tsv\", sep='\\t', header=0)\n",
        "dataset_numpy=dataset.to_numpy()\n",
        "x=dataset_numpy[:,:-2]\n",
        "y=dataset_numpy[:,-2:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "those are 2 function that allows to see that solving with good performances just one of the two tasks would be sufficient to solve also the other task so use it just to see this think not for the true evaluation of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just for seeing the strange results described with the y values in the train set\n",
        "#add cpa to x for the classification\n",
        "x=np.hstack((x,y[:,1].reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just for seeing the strange results described with the y values in the train set\n",
        "#add collision to the regression\n",
        "x=np.hstack((x,y[:,0].reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "add new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#if you want\n",
        "#add the cosine of the angle from the north as a new feature \n",
        "# add the distance in x and y afrom the uavs to their target a\n",
        "print(f\"shape before {x.shape}\")\n",
        "for i in range(0, x.shape[1],7):\n",
        "    distance_x=x[:,i+1]-x[:,i+5]\n",
        "    distance_y=x[:,i+2]-x[:,i+6]\n",
        "    cosine_north=np.cos(x[:,i])\n",
        "    x=np.append(x, np.array([distance_x]).T, axis=1)\n",
        "    x=np.append(x, np.array([distance_y]).T, axis=1)\n",
        "    x=np.append(x, np.array([cosine_north]).T, axis=1)\n",
        "print(f\"shape after {x.shape}\")  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKDdyoIAcfmZ"
      },
      "source": [
        "use the minmax scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7lqnkF5hDaN"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(x)\n",
        "x=scaler.transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk10t--Achxo"
      },
      "source": [
        "use the standard scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN1QjNSGcZOS"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x=scaler.transform(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bmQSJSSr_qr"
      },
      "source": [
        "if you want to normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7oZRiAAsC5a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "x=normalize(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz4B9FtkuEMK"
      },
      "source": [
        "trying to use pca features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCQ-nkbEt_pE"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=4)\n",
        "x = pca.fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "trying to use svd features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd=TruncatedSVD(n_components=15)\n",
        "x=svd.fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knLOg3BB0YxM"
      },
      "source": [
        "trying to extract nonlinear features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US4tmNO70b4O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "x=QuantileTransformer().fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "split the dataset in train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,random_state=45)\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "# print(f\"the classes and the number of occurrences in the train set are {np.unique(y_train, return_counts=True)}\")\n",
        "# print(f\"the classes and the number of occurrences in the test set are {np.unique(y_test, return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import *\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#make a pipeline\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
        "\n",
        "#grid search\n",
        "param_grid = {\n",
        "                'svc__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "                'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer(), MaxAbsScaler(), None],\n",
        "            }\n",
        "grid = GridSearchCV(pipe, param_grid, refit=True, verbose=1)\n",
        "grid.fit(x_train, y_train[:,0])\n",
        "\n",
        "print(grid.best_params_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "print(grid.score(x_test, y_test[:,0]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "find the closest point in cosine similarity and set the last row to the closest point in the train last row as described in the report to see if something could be done better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this can add as x the closest if you want to try the classification helping the model \n",
        "#find the closest point to each point in the train set and add it as a feature to the x test\n",
        "l_test=[]\n",
        "for i in range(x_test.shape[0]):\n",
        "    index=0\n",
        "    max_similar=0\n",
        "    count=0\n",
        "    for j in range(x_train.shape[0]):\n",
        "        cossim=cosine_similarity(x_test[i].reshape(1,-1), x_train[j].reshape(1,-1))\n",
        "        count+=cossim\n",
        "        if cossim> max_similar:\n",
        "            max_similar=cossim\n",
        "            index=j\n",
        "    l_test.append(y_train[index][1])\n",
        "    # print(max_similar, count/x_train.shape[0])\n",
        "l_test=np.array(l_test)\n",
        "x_test=np.hstack((x_test,l_test.reshape(-1,1)))\n",
        "print(\"finished adding the test features\")\n",
        "x_train=np.hstack((x_train,y_train))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model=RandomForestRegressor(n_estimators=100, max_depth=10, random_state=0)\n",
        "model.fit(x_train,y_train[:,1])\n",
        "l_test=model.predict(x_test)\n",
        "x_test=np.hstack((x_test,l_test.reshape(-1,1)))\n",
        "l_train=model.predict(x_train)\n",
        "x_train=np.hstack((x_train,l_train.reshape(-1,1)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "compute class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this computest class weights for the classificaiton but its not useful \n",
        "#could be used to balance the outputs of the model to give less importance to the majority class\n",
        "#and more importance to the minority class\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train[:,0]), \n",
        "    y=y_train[:,0]\n",
        "    )\n",
        "class_weight=dict(enumerate(class_weights))\n",
        "sample_weight=np.array([class_weight[y_train[i,0]] for i in range(y_train.shape[0])])\n",
        "class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "number of samples for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#count the number of occurrences of each class\n",
        "print(f\"the classes and the number of occurrences in the train set are {np.unique(y_train[:,0], return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "use smota with a mix of under and over sampling if you want to use this for the classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#you can change the parameters to see the different performances\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "#i let the last classas it is since they are reaaaaly low in n of samples\n",
        "over_sampler = RandomOverSampler(sampling_strategy={}, random_state=0)\n",
        "under_sampler = RandomUnderSampler(sampling_strategy={0:100,1:100}, random_state=0)\n",
        "smote=SMOTE(sampling_strategy={2:100,3:100},random_state=100)\n",
        "\n",
        "\n",
        "x_train,y_train=under_sampler.fit_resample(x_train,y_train[:,0])\n",
        "x_train,y_train=over_sampler.fit_resample(x_train,y_train)\n",
        "x_train,y_train=smote.fit_resample(x_train,y_train)\n",
        "\n",
        "y_train=y_train.reshape(-1,1)\n",
        "print(f\"the classes and the number of occurrences in the train set are {np.unique(y_train, return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asK65MxYhDaR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a list of models with their performances metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test all classification models\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.ensemble import RUSBoostClassifier\n",
        "from imblearn.ensemble import EasyEnsembleClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "#import mlp classifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "models=[\n",
        "tree.DecisionTreeClassifier(),\n",
        "RandomForestClassifier(),\n",
        "AdaBoostClassifier(),\n",
        "# GradientBoostingClassifier(),\n",
        "LogisticRegression(),\n",
        "SVC(),\n",
        "KNeighborsClassifier(),\n",
        "GaussianNB(),\n",
        "BaggingClassifier(),\n",
        "BalancedBaggingClassifier(),\n",
        "BalancedRandomForestClassifier(),\n",
        "# RUSBoostClassifier(),\n",
        "# EasyEnsembleClassifier(),\n",
        "# MLPClassifier(),\n",
        "DummyClassifier( strategy='most_frequent')\n",
        "]\n",
        "\n",
        "classificationResults(x_train,x_test,y_train,y_test,models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "over_sampler = RandomOverSampler(sampling_strategy={4:7})\n",
        "\n",
        "\n",
        "imba_pipeline = make_pipeline(RandomOverSampler(sampling_strategy={4:7}),SMOTE(random_state=42), \n",
        "                              RandomForestClassifier(n_estimators=100, random_state=13))\n",
        "cross_val_score(imba_pipeline, x_train, y_train[:,0], scoring='f1_macro', cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "model=DummyClassifier(strategy='most_frequent')\n",
        "model.fit(x_train,y_train[:,0])\n",
        "classificationResults(x_train,x_test,y_train,y_test,[model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import tree\n",
        "\n",
        "imba_pipeline = make_pipeline(RandomOverSampler(sampling_strategy={4:7}),SMOTE(random_state=42), \n",
        "                            tree.DecisionTreeClassifier())\n",
        "params={\n",
        "    \"max_depth\": [3,5,10,15,20,None],\n",
        "    \"min_samples_split\": [2,5,7,10],\n",
        "    \"min_samples_leaf\": [1,2,5],\n",
        "    \"ccp_alpha\": [0,0.00001,0.0001,0.0002,0.0005,0.001],\n",
        "    }\n",
        "new_params = {'decisiontreeclassifier__' + key: params[key] for key in params}\n",
        "grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=5, scoring='f1_macro',\n",
        "                        return_train_score=True)\n",
        "grid_imba.fit(x_train, y_train[:,0])\n",
        "print(grid_imba.best_params_)\n",
        "print(grid_imba.best_score_)\n",
        "\n",
        "# model=tree.DecisionTreeClassifier(**clf.best_params_)\n",
        "# classificationResults(x_train,x_test,y_train,y_test,[model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "\n",
        "imba_pipeline = make_pipeline(RandomOverSampler(sampling_strategy={4:7}),SMOTE(random_state=42), \n",
        "                            SVC())\n",
        "params={\n",
        "    'C':[1, 10],\n",
        "    'gamma':[1, 10],\n",
        "    'kernel': ['rbf', 'poly', 'linear',\"sigmoid\"],\n",
        "    'degree': [2, 3, 4,5],\n",
        "    'class_weight': ['balanced', None],\n",
        "\n",
        "    }\n",
        "new_params = {'svc__' + key: params[key] for key in params}\n",
        "grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=5, scoring='f1_macro',\n",
        "                        return_train_score=True)\n",
        "grid_imba.fit(x_train, y_train[:,0])\n",
        "print(grid_imba.best_params_)\n",
        "print(grid_imba.best_score_)\n",
        "\n",
        "# model=tree.DecisionTreeClassifier(**clf.best_params_)\n",
        "# classificationResults(x_train,x_test,y_train,y_test,[model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "                # 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "                'kernel': ['rbf', 'linear', 'poly'],\n",
        "                # 'degree':[1,2,3,4,5,6,7,8,9,10],\n",
        "                # 'class_weight':['balanced',None],\n",
        "                # 'decision_function_shape':['ovo','ovr']\n",
        "                }\n",
        "model=SVC()\n",
        "\n",
        "clf = GridSearchCV(model, parameters,scoring='f1_macro',cv=5)\n",
        "clf.fit(x, y[:,0])\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "\n",
        "\n",
        "model=tree.DecisionTreeClassifier(**clf.best_params_)\n",
        "classificationResults(x_train,x_test,y_train,y_test,[model])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import random parametes search\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#random hyperparameters search for svm \n",
        "model= SVC()\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "                'kernel': ['rbf', 'linear', 'poly'],\n",
        "                # 'degree':[1,2,3,4,5,6,7,8,9,10],\n",
        "                # 'class_weight':['balanced',None],\n",
        "                # 'decision_function_shape':['ovo','ovr']\n",
        "                }\n",
        "random_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, verbose=2, random_state=42, n_jobs = -1)\n",
        "random_search.fit(x_train, y_train[:,0])\n",
        "print(random_search.best_params_)\n",
        "print(random_search.best_score_)\n",
        "print(random_search.best_estimator_)\n",
        "# classificationResults(x_train,x_test,y_train,y_test,random_search.best_estimator_)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "parameters={\n",
        "    'n_estimators': [50,70,100,120],\n",
        "    \"max_depth\": range(2,20),\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"min_samples_leaf\"=range(1,10),\n",
        "    \"min_samples_split\": range(1,10),\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
        "    \"class_weight\":['balanced',None]\n",
        "    }\n",
        "model=RandomForestClassifier()\n",
        "\n",
        "clf = GridSearchCV(model, parameters)\n",
        "clf.fit(x_train, y_train[:,0])\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLSKxw0-hDaT"
      },
      "outputs": [],
      "source": [
        "#test all regression models\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "models=[RandomForestRegressor(),AdaBoostRegressor(),GradientBoostingRegressor(),\\\n",
        "LinearRegression(),SVR(),\n",
        "KNeighborsRegressor(),\n",
        "DecisionTreeRegressor()]\n",
        "regressionResults(x_train,x_test,y_train,y_test,models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import random parametes search\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#random hyperparameters search for svm \n",
        "model= SVC()\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
        "                'kernel': ['rbf', 'linear', 'poly', 'rbf', 'linear'],\n",
        "                'degree':[1,2,3,4,5,6,7,8,9,10],\n",
        "                'class_weight':['balanced',None],\n",
        "                'decision_function_shape':['ovo','ovr']\n",
        "                }\n",
        "random_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, verbose=2, random_state=42, n_jobs = -1)\n",
        "random_search.fit(x_train, y_train[:,0])\n",
        "print(random_search.best_params_)\n",
        "print(random_search.best_score_)\n",
        "print(random_search.best_estimator_)\n",
        "# classificationResults(x_train,x_test,y_train,y_test,random_search.best_estimator_)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "parameters={\n",
        "    'n_estimators': [50,70,100,120],\n",
        "    \"max_depth\": range(2,20),\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"min_samples_leaf\"=range(1,10),\n",
        "    \"min_samples_split\": range(1,10),\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
        "    \"class_weight\":['balanced',None]\n",
        "    }\n",
        "model=RandomForestClassifier()\n",
        "\n",
        "clf = GridSearchCV(model, parameters)\n",
        "clf.fit(x_train, y_train[:,0])\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=RandomForestClassifier(class_weight=class_weight)\n",
        "classificationResults(x_train,x_test,y_train,y_test,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=RandomForestClassifier(class_weight='balanced')\n",
        "classificationResults(x_train,x_test,y_train,y_test,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLQ_rsf2-gbh",
        "outputId": "e4b78712-6309-4d65-febe-3495d85ac3e2"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn import svm\n",
        "# parameters={'kernel':('linear', 'poly' ,\"rbf\",\"sigmoid\"),'gamma':[\"scale\",\"auto\"],'class_weight':[None,\"balanced\"]}\n",
        "\n",
        "\n",
        "# model=svm.SVC()\n",
        "\n",
        "# clf = GridSearchCV(model, parameters)\n",
        "# clf.fit(x_train, y_train[:,0])\n",
        "\n",
        "# print(clf.best_params_)\n",
        "# print(clf.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6l3Sm3jlfq",
        "outputId": "7fcb7fb5-5dcf-4a79-fbb4-98c13d8a1c81"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# model=AdaBoostClassifier()\n",
        "# print(f\" those are the parameters to optimize {model.get_params()}\")\n",
        "# parameters={\"n_estimators\":[10,5,20,50,100,200,500,1000],\"learning_rate\":[0.01,0.001,0.0001,0.05,0.1,0.2,0.5,1]}\n",
        "# clf = GridSearchCV(model, parameters)\n",
        "# clf.fit(x_train, y_train[:,0])\n",
        "\n",
        "# print(clf.best_params_)\n",
        "# print(clf.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLSKxw0-hDaT"
      },
      "outputs": [],
      "source": [
        "#test all regression models\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import TweedieRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "models=[RandomForestRegressor(),\n",
        "AdaBoostRegressor(),\n",
        "# GradientBoostingRegressor(),\n",
        "LinearRegression(),\n",
        "SVR(),\n",
        "BayesianRidge(),\n",
        "# TweedieRegressor(),\n",
        "KNeighborsRegressor(),\n",
        "DecisionTreeRegressor()]\n",
        "regressionResults(x_train,x_test,y_train,y_test,models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters={\n",
        "    'n_estimators': [10000],\n",
        "    \"max_depth\": [100],\n",
        "}\n",
        "model=RandomForestRegressor()\n",
        "clf = GridSearchCV(model, parameters,scoring=\"neg_mean_squared_error\",cv=5)\n",
        "clf.fit(x_train, y_train[:,1])\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "print(clf.best_estimator_)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regressionResults(x_train,x_test,y_train,y_test,clf.best_estimator_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "32b23772a312cabd037b51ffca153a178270ce00afd94da1694114bd3650a0bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
